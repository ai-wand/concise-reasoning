{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def process_episode(episode, results_folder):\n",
    "    # each episode is a list of \"prompts\" (one or more \"prompts\" item)\n",
    "    lengths_list = []  # list of lengths for all prompts in the episode\n",
    "    rewards_list = []  # list of rewards for all prompts in the episode\n",
    "    for episode_prompts_file in episode:\n",
    "        file = os.path.join(results_folder, episode_prompts_file)\n",
    "        x = np.load(file)\n",
    "        lengths = x[0].tolist()\n",
    "        rewards = x[1].tolist()  # --> -1 for no box; -0.5 for box but wrong; 1 for correct\n",
    "        lengths_list.extend(lengths)\n",
    "        rewards_list.extend(rewards)\n",
    "    return lengths_list, rewards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 2 replication -- varying difficulty\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "results_folders = {\n",
    "    'p=0.0625': '../train/checkpoints/R1_on_aime_p_0.0625/experiments',\n",
    "    'p=0.125': '../train/checkpoints/R1_on_aime_p_0.125/experiments',\n",
    "    'p=0.25': '../train/checkpoints/R1_on_aime_p_0.25/experiments',\n",
    "    'p=0.375': '../train/checkpoints/R1_on_aime_p_0.375/experiments'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name in results_folders:\n",
    "    results_folder = results_folders[name]\n",
    "    print(name)\n",
    "    episodes = {}\n",
    "    for file in os.listdir(results_folder):\n",
    "        episode = int(file.split('_')[0].split('episode')[1])\n",
    "        episodes[episode] = []\n",
    "\n",
    "    for episode in episodes:\n",
    "        for file in os.listdir(results_folder):\n",
    "            if file.startswith(f'episode{episode}_'):\n",
    "                episodes[episode].append(file)\n",
    "\n",
    "    mean_length_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for episode in range(len(episodes)):\n",
    "        ep_list = episodes[episode]  # list of prompts for this episode\n",
    "        lengths, rewards = process_episode(ep_list, results_folder)\n",
    "        avg_length = np.mean(lengths)\n",
    "        rewards = [1 if item > 0 else 0 for item in rewards]\n",
    "        acc = 100.0 * sum(rewards) / len(rewards)\n",
    "        mean_length_list.append(avg_length)\n",
    "        accuracy_list.append(acc)\n",
    "    results[name] = [accuracy_list, mean_length_list]\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.close('all')\n",
    "plt.style.use(\"./publication.mplstyle\")\n",
    "smoothing_window = 50\n",
    "steps = np.arange(1, 1800 + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(3, 4), sharex=True, gridspec_kw={'hspace': 0.14})\n",
    "ax1, ax2 = axes\n",
    "\n",
    "for name in results:\n",
    "    print(name)\n",
    "    [accuracy_list, mean_length_list] = results[name]\n",
    "    # smoothing the data\n",
    "    acc = pd.Series(accuracy_list)\n",
    "    acc = acc.rolling(window=smoothing_window, min_periods=1, center=True).mean().tolist()\n",
    "    mlen = pd.Series(mean_length_list)\n",
    "    mlen = mlen.rolling(window=smoothing_window, min_periods=1, center=True).mean().tolist()\n",
    "    xsize = min(len(acc), len(steps))\n",
    "    ax1.plot(steps[:xsize], acc[:xsize], ls='-', lw=1, label=name, alpha=0.8)\n",
    "    ax2.plot(steps[:xsize], mlen[:xsize], ls='-', lw=1, alpha=0.8)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    \n",
    "ax1.set_xticks([1] + np.arange(300, len(steps)+1, 300).tolist())\n",
    "ax1.set_xlim(-1, len(steps))\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_ylabel(\"Response Length (tokens)\")\n",
    "ax2.set_xlabel(\"Steps\")\n",
    "\n",
    "ax1.set_yticks(np.arange(0, 101, 10))\n",
    "ax1.set_ylim(0, 100)\n",
    "ax2.set_yticks(np.arange(3000, 20000, 2000))\n",
    "ax2.set_ylim(3000, 20000)\n",
    "\n",
    "def format_ticks(value, _):\n",
    "    return f'{int(value/1000)}K'\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 0.94), ncol=3, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Figure 3 replication for R1-1.5B\n",
    "# Running this cell takes about 15 minutes\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "pretrain = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrain)\n",
    "\n",
    "def get_len(response_list):\n",
    "    tokenized_outputs = tokenizer(response_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    lengths = (tokenized_outputs[\"input_ids\"] != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "    return lengths\n",
    "\n",
    "def prepare_results(checkpoint_folder, data_name):\n",
    "    tags = [item for item in os.listdir(checkpoint_folder) if 'global_step' in item]\n",
    "    tags = sorted(tags, key=lambda x: int(re.search(r'\\d+$', x).group()))  # making sort incremental\n",
    "    tags = [tag for tag in tags if int(re.search(r'\\d+$', tag).group()) <= 148]\n",
    "    eval_folders = [os.path.join(checkpoint_folder, tag, 'eval', data_name) for tag in tags]\n",
    "    accuracy_list = []\n",
    "    mean_length_list = []\n",
    "    mean_true_len_list = []\n",
    "    mean_false_len_list = []\n",
    "    for folder in eval_folders:\n",
    "        print(folder)\n",
    "        try:\n",
    "            files = os.listdir(folder)\n",
    "            files = [file for file in files if 'metrics' not in file]\n",
    "            assert len(files) == 1\n",
    "            file = os.path.join(folder, files[0])\n",
    "            data = []\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "            df = pd.DataFrame(data)\n",
    "            df['qwen_avg_score'] = df['score'].apply(np.mean)\n",
    "            df['response_lengths'] = df['code'].apply(get_len)\n",
    "            df['mean_lengths'] = df['response_lengths'].apply(lambda x: int(np.mean(x)))\n",
    "            df['true_len'] = df.apply(lambda row: [l for l, c in zip(row['response_lengths'], row['score']) if c], axis=1)\n",
    "            df['false_len'] = df.apply(lambda row: [l for l, c in zip(row['response_lengths'], row['score']) if not c], axis=1)\n",
    "            df['mean_true_len'] = df['true_len'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "            df['mean_false_len'] = df['false_len'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)  # zero if empty\n",
    "            accuracy_list.append(float(np.mean(df['qwen_avg_score'])))\n",
    "            mean_length_list.append(int(np.mean(df['mean_lengths'])))\n",
    "            mean_true_len_list.append(int(np.mean([x for x in df['mean_true_len'] if x != 0])))\n",
    "            mean_false_len_list.append(int(np.mean([x for x in df['mean_false_len'] if x != 0])))\n",
    "        except:\n",
    "            print('>>> missing data')\n",
    "            accuracy_list.append(accuracy_list[-1])\n",
    "            mean_length_list.append(mean_length_list[-1])\n",
    "            mean_true_len_list.append(mean_true_len_list[-1])\n",
    "            mean_false_len_list.append(mean_false_len_list[-1])\n",
    "    return accuracy_list, mean_length_list, mean_true_len_list, mean_false_len_list\n",
    "\n",
    "\n",
    "benchmarks = ['math500', 'aime24', 'amc23', 'mmlu_stem']\n",
    "checkpoint_folder = '../train/checkpoints/R1_on_math/_actor/'\n",
    "\n",
    "results = {}\n",
    "for benchmark in benchmarks:\n",
    "    results[benchmark] = prepare_results(checkpoint_folder, benchmark)\n",
    "\n",
    "\n",
    "lw_list = [1, 1, 1, 1]\n",
    "ls_list = ['-', ':', '--', '-.']\n",
    "palette = ['#0C5DA5', '#FF9500', '#00B945', '#FF2C00', '#845B97', '#474747', '#9e9e9e']\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(3, 3), sharex=True, gridspec_kw={'hspace': 0.1})  # Small gap added\n",
    "ax1, ax2 = axes\n",
    "\n",
    "benchmarks = ['math500', 'aime24', 'amc23', 'mmlu_stem']\n",
    "\n",
    "for benchmark_idx, benchmark in enumerate(benchmarks):\n",
    "    if benchmark == 'mmlu_stem':\n",
    "        continue\n",
    "    accuracy_list = np.array(results[benchmark][0]) * 100\n",
    "    mean_length_list = results[benchmark][1]\n",
    "    steps = np.arange(0, len(accuracy_list)) * 4\n",
    "    color = palette[0]\n",
    "    ls = ls_list[benchmark_idx]\n",
    "    ax1.plot(steps, accuracy_list, ls=ls, color=color, alpha=0.8)\n",
    "    ax2.plot(steps, mean_length_list, ls=ls, color=color, alpha=0.8)\n",
    "\n",
    "ax1.set_xlim(steps[0], steps[-1])\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_ylabel(\"Response Length (tokens)\")\n",
    "ax2.set_xlabel(\"Steps\")\n",
    "ax1.set_xticks(np.arange(0, steps[-1], 16).tolist())\n",
    "ax1.set_xlim(-1, steps[-1]+1)\n",
    "ax1.set_yticks(np.arange(10, 101, 10))\n",
    "ax1.set_ylim(10, 100)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "def format_ticks(value, _):\n",
    "    return f'{int(value/1000)}K'\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "model_legend = [mlines.Line2D([], [], color=palette[i], lw=1.5, label=exp.split('_8')[0].replace('_', ' ')+'B') for i, exp in enumerate(results.keys())]\n",
    "benchmark_legend = [mlines.Line2D([], [], color='black', lw=1.5, ls=ls_list[i], label=benchmarks[i]) for i in range(len(benchmarks))]\n",
    "# fig.legend(handles=model_legend, loc=\"upper left\", bbox_to_anchor=(0.2, 1.07), frameon=False, title=\"Models\")\n",
    "fig.legend(handles=benchmark_legend, loc=\"upper right\", bbox_to_anchor=(0.8, 1.07), frameon=False, title=\"Benchmarks\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
